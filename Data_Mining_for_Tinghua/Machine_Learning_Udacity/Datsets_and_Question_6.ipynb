{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2017.4.15 晚\n",
    "## 准确率与训练集大小\n",
    "一般情况下，训练越大，准确率越高，在某一一个值附近趋于平缓，不再增加。但实际中，有时候获取数据的代价非常昂贵，甚至只能获取一小部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146\n",
      "21\n",
      "the number of POi 18\n",
      "the number of poi_names 37\n",
      "8682716\n",
      "103559793\n",
      "2424083\n",
      "95\n",
      "111\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "enron_data = pickle.load(open(\"E:\\\\machine-learning\\\\Data_Mining_for_Tinghua\\\\Machine_Learning_Udacity\\\\ud120-projects\\\\final_project\\\\final_project_dataset.pkl\", \"r\"))\n",
    "\n",
    "# 查看人数\n",
    "print len(enron_data)\n",
    "# 查看整个数据集\n",
    "#print enron_data\n",
    "\n",
    "# 查看每个人具有的特征数\n",
    "print len(enron_data[\"SKILLING JEFFREY K\"])\n",
    "\n",
    "\n",
    "# 安然数据中POI的数目\n",
    "count = 0\n",
    "person_names = enron_data.keys()\n",
    "for person_name in person_names:\n",
    "    if enron_data[person_name][\"poi\"]==1:\n",
    "        count += 1\n",
    "print 'the number of POi',count\n",
    "\n",
    "# 兑入poi_names 并统计poi_names 条数 竟然有一行网址和空格！所以是35\n",
    "file_path = 'E:\\\\machine-learning\\\\Data_Mining_for_Tinghua\\\\Machine_Learning_Udacity\\\\ud120-projects\\\\final_project\\\\poi_names.txt'\n",
    "with open (file_path,'r') as f1:\n",
    "    context = f1.readlines()\n",
    "print 'the number of poi_names',len(context)\n",
    "\n",
    "# James Prentice下的股票\n",
    "\n",
    "# person_names = sorted(enron_data.keys())\n",
    "# for person_name in person_names:\n",
    "#     print person_name\n",
    "    \n",
    "enron_data['PRENTICE JAMES']\n",
    "enron_data['PRENTICE JAMES']['total_stock_value']\n",
    "\n",
    "# # 来自 Wesley Colwell 的发给嫌疑人的电子邮件\n",
    "enron_data['COLWELL WESLEY']   \n",
    "\n",
    "# Jeffrey K Skilling 行使的股票期权价值是多少？\n",
    "enron_data['SKILLING JEFFREY K']\n",
    "\n",
    "# 这三个人（Lay、Skilling 和 Fastow）当中，谁拿回家的钱最多（“total_payments”特征的最大值）？\n",
    "print enron_data['SKILLING JEFFREY K']['total_payments']\n",
    "print enron_data['LAY KENNETH L']['total_payments']\n",
    "print enron_data['FASTOW ANDREW S']['total_payments']\n",
    "\n",
    "# 此数据集中有多少雇员有量化的工资？已知的邮箱地址是否可用？\n",
    "\n",
    "count_salary = 0\n",
    "count_email = 0\n",
    "person_names = enron_data.keys()\n",
    "for person_name in person_names:\n",
    "    if enron_data[person_name][\"salary\"]!='NaN':\n",
    "        count_salary += 1\n",
    "    if enron_data[person_name][\"email_address\"]!='NaN':\n",
    "        count_email += 1\n",
    "print count_salary\n",
    "print count_email\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 我们编写了一些辅助函数（tools/feature_format.py 中的 featureFormat() 和 targetFeatureSplit()），\n",
    "#它们可以获取特征名的列表和数据字典，然后返回 numpy 数组。\n",
    "\n",
    "#如果特征没有某个特定人员的值，此函数还会用 0（零）替换特征值。\n",
    "#!/usr/bin/python\n",
    "\n",
    "\"\"\" \n",
    "    A general tool for converting data from the\n",
    "    dictionary format to an (n x k) python list that's \n",
    "    ready for training an sklearn algorithm\n",
    "\n",
    "    n--no. of key-value pairs in dictonary\n",
    "    k--no. of features being extracted\n",
    "\n",
    "    dictionary keys are names of persons in dataset\n",
    "    dictionary values are dictionaries, where each\n",
    "        key-value pair in the dict is the name\n",
    "        of a feature, and its value for that person\n",
    "\n",
    "    In addition to converting a dictionary to a numpy \n",
    "    array, you may want to separate the labels from the\n",
    "    features--this is what targetFeatureSplit is for\n",
    "\n",
    "    so, if you want to have the poi label as the target,\n",
    "    and the features you want to use are the person's\n",
    "    salary and bonus, here's what you would do:\n",
    "\n",
    "    feature_list = [\"poi\", \"salary\", \"bonus\"] \n",
    "    data_array = featureFormat( data_dictionary, feature_list )\n",
    "    label, features = targetFeatureSplit(data_array)\n",
    "\n",
    "    the line above (targetFeatureSplit) assumes that the\n",
    "    label is the _first_ item in feature_list--very important\n",
    "    that poi is listed first!\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def featureFormat( dictionary, features, remove_NaN=True, remove_all_zeroes=True, remove_any_zeroes=False, sort_keys = False):\n",
    "    \"\"\" convert dictionary to numpy array of features\n",
    "        remove_NaN = True will convert \"NaN\" string to 0.0\n",
    "        remove_all_zeroes = True will omit any data points for which\n",
    "            all the features you seek are 0.0\n",
    "        remove_any_zeroes = True will omit any data points for which\n",
    "            any of the features you seek are 0.0\n",
    "        sort_keys = True sorts keys by alphabetical order. Setting the value as\n",
    "            a string opens the corresponding pickle file with a preset key\n",
    "            order (this is used for Python 3 compatibility, and sort_keys\n",
    "            should be left as False for the course mini-projects).\n",
    "        NOTE: first feature is assumed to be 'poi' and is not checked for\n",
    "            removal for zero or missing values.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    return_list = []\n",
    "\n",
    "    # Key order - first branch is for Python 3 compatibility on mini-projects,\n",
    "    # second branch is for compatibility on final project.\n",
    "    if isinstance(sort_keys, str):\n",
    "        import pickle\n",
    "        keys = pickle.load(open(sort_keys, \"rb\"))\n",
    "    elif sort_keys:\n",
    "        keys = sorted(dictionary.keys())\n",
    "    else:\n",
    "        keys = dictionary.keys()\n",
    "\n",
    "    for key in keys:\n",
    "        tmp_list = []\n",
    "        for feature in features:\n",
    "            try:\n",
    "                dictionary[key][feature]\n",
    "            except KeyError:\n",
    "                print \"error: key \", feature, \" not present\"\n",
    "                return\n",
    "            value = dictionary[key][feature]\n",
    "            if value==\"NaN\" and remove_NaN:\n",
    "                value = 0\n",
    "            tmp_list.append( float(value) )\n",
    "\n",
    "        # Logic for deciding whether or not to add the data point.\n",
    "        append = True\n",
    "        # exclude 'poi' class as criteria.\n",
    "        if features[0] == 'poi':\n",
    "            test_list = tmp_list[1:]\n",
    "        else:\n",
    "            test_list = tmp_list\n",
    "        ### if all features are zero and you want to remove\n",
    "        ### data points that are all zero, do that here\n",
    "        if remove_all_zeroes:\n",
    "            append = False\n",
    "            for item in test_list:\n",
    "                if item != 0 and item != \"NaN\":\n",
    "                    append = True\n",
    "                    break\n",
    "        ### if any features for a given data point are zero\n",
    "        ### and you want to remove data points with any zeroes,\n",
    "        ### handle that here\n",
    "        if remove_any_zeroes:\n",
    "            if 0 in test_list or \"NaN\" in test_list:\n",
    "                append = False\n",
    "        ### Append the data point if flagged for addition.\n",
    "        if append:\n",
    "            return_list.append( np.array(tmp_list) )\n",
    "\n",
    "    return np.array(return_list)\n",
    "\n",
    "\n",
    "def targetFeatureSplit( data ):\n",
    "    \"\"\" \n",
    "        given a numpy array like the one returned from\n",
    "        featureFormat, separate out the first feature\n",
    "        and put it into its own list (this should be the \n",
    "        quantity you want to predict)\n",
    "\n",
    "        return targets and features as separate lists\n",
    "\n",
    "        (sklearn can generally handle both lists and numpy arrays as \n",
    "        input formats when training/predicting)\n",
    "    \"\"\"\n",
    "\n",
    "    target = []\n",
    "    features = []\n",
    "    for item in data:\n",
    "        target.append( item[0] )\n",
    "        features.append( item[1:] )\n",
    "\n",
    "    return target, features\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "0.143835616438\n",
      "18\n",
      "0\n",
      "0.0\n",
      "the number of count_stock 0\n"
     ]
    }
   ],
   "source": [
    "# （当前的）E+F 数据集中有多少人的薪酬总额被设置了“NaN”？数据集中这些人的比例占多少？\n",
    "count_nan = 0\n",
    "person_names = enron_data.keys()\n",
    "for person_name in person_names:\n",
    "    if enron_data[person_name][\"total_payments\"]=='NaN':\n",
    "        count_nan += 1\n",
    "print count_nan\n",
    "print (count_nan/146.0)\n",
    "\n",
    "# E+F 数据集中有多少 POI 的薪酬总额被设置了“NaN”？这些 POI 占多少比例？\n",
    "\n",
    "count_na = 0\n",
    "count = 0\n",
    "poi_members = []\n",
    "person_names = enron_data.keys()\n",
    "for person_name in person_names:\n",
    "    if enron_data[person_name][\"poi\"]==1:\n",
    "        poi_members.append(person_name)\n",
    "        count += 1\n",
    "for poi_member in poi_members:\n",
    "    if enron_data[poi_member]['total_payments'] == 'NaN':\n",
    "        count_na += 1\n",
    "print count\n",
    "print count_na\n",
    "            \n",
    "print count_na/float(count) # 当时还不相信自己的结果\n",
    "\n",
    "\n",
    "# 如果你再次添加了全是 POI 的 10 个数据点，并且对这些雇员的薪酬总额设置了“NaN”，你刚才计算的数字会发生变化。\n",
    "# 数据集中这些人的数量变成了多少？薪酬总额被设置了“NaN”的雇员数变成了多少？\n",
    "\n",
    "\n",
    "# 数据集中的 POI 数量变成了多少？股票总值被设置了“NaN”的 POI 占多少比例？\n",
    "count_stock = 0\n",
    "for poi_member in poi_members:\n",
    "    if enron_data[poi_member]['total_stock_value'] == 'NaN':\n",
    "        count_stock += 1\n",
    "print 'the number of count_stock',count_stock\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读写文件巩固\n",
    "读写文件就是请求操作系统打开一个文件对象（通常称为文件描述符），然后，通过操作系统提供的接口从这个文件对象中读取数据（读文件），或者把数据写入这个文件对象（写文件）。\n",
    "\n",
    "### 读文件 \n",
    "读文件是将磁盘上的文件读到内存中；默认都是读取文本文件，并且是UTF-8编码的文本文件。要读取二进制文件，比如图片、视频等等，用'rb'模式打开文件即可\n",
    "+ **read** read()方法可以一次读取文件的全部内容，Python把内容读到内存，用一个str对象表\n",
    "\n",
    "```python\n",
    "# example for read\n",
    "with open('/path/to/file', 'r') as f:\n",
    "    print(f.read())\n",
    "```\n",
    "+ **read(size)** 如果文件内容过大，可以反复调用read(size)方法，每次最多读取size个字节的内容\n",
    "+ **readline** 调用readline()可以每次读取一行内容，\n",
    "+ **readlines** 调用readlines()一次读取所有内容并按行返回list\n",
    "+ **读非utf-8编码** 需要给open()函数传入encoding参数，例如，读取GBK编码的文件\n",
    "`f = open('/Users/michael/gbk.txt', 'r', encoding='gbk')`\n",
    "+ **UnicodeDecodeError** 遇到到有些编码不规范的文件，你可能会遇到UnicodeDecodeError，因为在文本文件中可能夹杂了一些非法编码的字符。遇到这种情况，open()函数还接收一个errors参数，表示如果遇到编码错误后如何处理。最简单的方式是直接忽略：\n",
    "`f = open('/Users/michael/gbk.txt', 'r', encoding='gbk', errors='ignore')\n",
    "`\n",
    "### 写文件\n",
    "通过操作系统将文件写到磁盘中，操作系统往往不会立刻把数据写入磁盘，而是放到内存缓存起来，空闲的时候再慢慢写入。只有调用close()方法时，操作系统才保证把没有写入的数据全部写入磁盘。忘记调用close()的后果是数据可能只写了一部分到磁盘，剩下的丢失了。所以，还是用with语句来得保险.要写入特定编码的文本文件，请给open()函数传入encoding参数，将字符串自动转换成指定编码。\n",
    "\n",
    "```python\n",
    "with open('/Users/michael/test.txt', 'w') as f:\n",
    "    f.write('Hello, world!')\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
